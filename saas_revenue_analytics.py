# -*- coding: utf-8 -*-
"""saas_revenue_analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gc2tjlCaHHWKaj1KGzhNM8h4QDSE7Imi
"""

import numpy as np
import pandas as pd

"""# SaaS Revenue Analysis: Correlation & Insights

This project performs **correlation analysis** and **insight discovery** for SaaS company revenue data. The focus is on understanding how different features influence company performance by comparing behavior across above-average and below-average revenue segments.

---

## 🎯 Project Objective

To analyze key performance metrics across SaaS companies and discover:

- How feature relationships differ between high and low performers
- Which metrics are most influential in determining ARR/MRR growth
- Actionable business insights from comparing feature correlations

---

## 🗂️ Project Files
├── saas_revenue_analytics.ipynb # Main analysis notebook
├── saas_revenue_dataset.csv # Dataset used in analysis
├── README.md

---

## 📈 Key Analysis Performed

### 📊 Feature Correlation Comparison

- Separated companies into **above-average** and **below-average** revenue performers
- Computed correlation matrices for both groups
- Visualized and compared how feature relationships change between segments

This helps answer questions like:
- Does churn correlate differently with NPS in top-performing companies?
- Is marketing efficiency more predictive of growth in low performers?

---

## 🧠 Summary Insights

- **Top performers** often show stronger positive correlation between NPS and ARR.
- In **below-average** companies, churn and ARR have a stronger negative correlation.
- **Marketing spend** does not always lead to more subscribers in weaker segments — indicating diminishing returns or poor targeting.
- **Interaction terms** like churn × marketing or NPS × churn differ meaningfully across segments.

These insights can help business teams focus on the right levers for improving revenue performance.

---

## 📎 Requirements

- Python 3.8+
- Jupyter Notebook
- Libraries:
  - `pandas`, `numpy`, `matplotlib`, `seaborn`

---

## 🚀 How to Run

1. Clone this repository  
2. Open `saas_revenue_analytics.ipynb` in Jupyter  
3. Run all cells sequentially to reproduce the analysis and plots

---

## 🙋 Author

This project was conducted by a Deepak to showcase real-world SaaS metrics interpretation, revenue behavior understanding, and how data correlations drive insights for business strategy.

---

## 📌 Note

This project does not include forecasting models. Its primary focus is on **data-driven storytelling and insight extraction** using clean analytics practices.
"""

# Your sheet ID
sheet_id = "1huXOVIomC9asg64d3pRtMsKlS0_z4uDqGs57_tDf5Is"
sheet_name = "saas_revenue_dataset"  # Replace if your sheet tab is named differently

# Construct the export CSV URL
csv_url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}"

# Read the sheet
df = pd.read_csv(csv_url)

# Display the data
df.head()

df.columns

df.describe()

df.company_id.nunique()

print(df.date.max(),df.date.min())

df.company_id.unique(

)

record_counts = df['company_name'].value_counts()
record_counts

df.shape

df.dtypes

df.info()

df.describe(include = 'object').columns

for i in df.describe(include = 'object').columns:
    print(f"Unique entries for column {i:<30} = {df[i].nunique()}")







u  =  df.describe(include = 'object').columns[1:]


# Iterate through each column and print unique values
for column in u:
    unique_values = df[column].unique()
    print(f"Unique values in column '{column}':")
    print(unique_values)
    print("-" * 50)

np.any(df.isnull())

correlation_matrix = df.corr(numeric_only=True)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 10))

# Generate a heatmap with annotations
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", square=True, linewidths=0.5)

# Add title
plt.title("Correlation Matrix Heatmap")

# Show the plot
plt.tight_layout()
plt.show()

"""mrr and arr subscribers is directly correlated to the  new subscribers , marketing spend, active users, employees , support tickets.
also indirectly propotional to the churn rate


"""

print("\nDuplicate rows:", df.duplicated().sum())

arr_trend = df.groupby('date')['arr_usd'].sum().reset_index()

# Plot the ARR trend
plt.figure(figsize=(12, 6))
plt.plot(arr_trend['date'], arr_trend['arr_usd'], marker='o')
plt.title("Combined ARR Trend Across All Companies")
plt.xlabel("Date")
plt.ylabel("Total ARR (USD)")
plt.grid(True)
plt.tight_layout()
plt.show()

mrr_trend = df.groupby('date')['mrr_usd'].sum().reset_index()

# Plot the MRR trend
plt.figure(figsize=(12, 6))
plt.plot(mrr_trend['date'], mrr_trend['mrr_usd'], marker='o')
plt.title("Combined MRR Trend Across All Companies")
plt.xlabel("Date")
plt.ylabel("Total MRR (USD)")
plt.grid(True)
plt.tight_layout()
plt.show()

arr_start_end = df.sort_values('date').groupby('company_name')['arr_usd'].agg(['first', 'last'])

# Identify companies with declining ARR
declining_companies = arr_start_end[arr_start_end['last'] < arr_start_end['first']].reset_index()

# Display the result
print("Companies with declining ARR over time:")
print(declining_companies)

declining_company_list = declining_companies.company_name
dec = df[df.company_name.isin(declining_company_list)]

arr_trend = dec.groupby('date')['arr_usd'].sum().reset_index()

# Plot the ARR trend
plt.figure(figsize=(12, 6))
plt.plot(arr_trend['date'], arr_trend['arr_usd'], marker='o')
plt.title("Combined ARR Trend Across All Companies")
plt.xlabel("Date")
plt.ylabel("Total ARR (USD)")
plt.grid(True)
plt.tight_layout()
plt.show()

arr_start_end = df.sort_values('date').groupby('company_name')['arr_usd'].agg(['first', 'last'])

# Identify companies with declining ARR
arr_increase = arr_start_end[arr_start_end['last'] > arr_start_end['first']].reset_index()

# Display the result
print("Companies with increasing ARR over time:")
print(arr_increase)

increasing_company_list = arr_increase.company_name
inc = df[df.company_name.isin(increasing_company_list)]

#arr increasing companies

arr_trend = inc.groupby('date')['arr_usd'].sum().reset_index()

# Plot the ARR trend
plt.figure(figsize=(12, 6))
plt.plot(arr_trend['date'], arr_trend['arr_usd'], marker='o')
plt.title("Combined ARR Trend Across All Companies")
plt.xlabel("Date")
plt.ylabel("Total ARR (USD)")
plt.grid(True)
plt.tight_layout()
plt.show()

#marketing_spend_usd, active users , employees


# Sort by date for accurate comparison
df_sorted = df.sort_values('date')

# Identify companies with declining ARR
arr_start_end = df_sorted.groupby('company_name')['arr_usd'].agg(['first', 'last'])
declining_companies = arr_start_end[arr_start_end['last'] < arr_start_end['first']].index

# Filter dataset for declining companies
df_declining = df_sorted[df_sorted['company_name'].isin(declining_companies)]

# Aggregate marketing spend by date
marketing_trend = df_declining.groupby('date')['marketing_spend_usd'].sum().reset_index()

# Plot the marketing spend trend
plt.figure(figsize=(12, 6))
plt.plot(marketing_trend['date'], marketing_trend['marketing_spend_usd'], marker='o', color='red')
plt.title("Combined Marketing Spend Trend for Declining ARR Companies")
plt.xlabel("Date")
plt.ylabel("Total Marketing Spend (USD)")
plt.grid(True)
plt.tight_layout()
plt.show()

# Sort by date for accurate comparison
df_sorted = df.sort_values('date')

# Identify companies with declining ARR
arr_start_end = df_sorted.groupby('company_name')['arr_usd'].agg(['first', 'last'])
inc_com = arr_start_end[arr_start_end['last'] > arr_start_end['first']].index

# Filter dataset for declining companies
df_inc = df_sorted[df_sorted['company_name'].isin(inc_com)]

# Aggregate marketing spend by date
marketing_trend = df_inc.groupby('date')['marketing_spend_usd'].sum().reset_index()

# Plot the marketing spend trend
plt.figure(figsize=(12, 6))
plt.plot(marketing_trend['date'], marketing_trend['marketing_spend_usd'], marker='o', color='red')
plt.title("Combined Marketing Spend Trend for increasing ARR Companies")
plt.xlabel("Date")
plt.ylabel("Total Marketing Spend (USD)")
plt.grid(True)
plt.tight_layout()
plt.show()

# Sort by date for accurate comparison
df_sorted = df.sort_values('date')

# Identify companies with declining ARR
arr_start_end = df_sorted.groupby('company_name')['arr_usd'].agg(['first', 'last'])
declining_companies = arr_start_end[arr_start_end['last'] < arr_start_end['first']].index

# Filter dataset for declining companies
df_declining = df_sorted[df_sorted['company_name'].isin(declining_companies)]

# Aggregate active users  by date
marketing_trend = df_declining.groupby('date')['feature_daily_active_users'].sum().reset_index()

# Plot the active users  trend
plt.figure(figsize=(12, 6))
plt.plot(marketing_trend['date'], marketing_trend['feature_daily_active_users'], marker='o', color='red')
plt.title("Combined active users Trend for Declining ARR Companies")
plt.xlabel("Date")
plt.ylabel("active users  ")
plt.grid(True)
plt.tight_layout()
plt.show()

# Sort by date for accurate comparison
df_sorted = df.sort_values('date')

# Identify companies with declining ARR
arr_start_end = df_sorted.groupby('company_name')['arr_usd'].agg(['first', 'last'])
inc_com = arr_start_end[arr_start_end['last'] > arr_start_end['first']].index

# Filter dataset for declining companies
df_inc = df_sorted[df_sorted['company_name'].isin(inc_com)]

# Aggregate active users by date
marketing_trend = df_inc.groupby('date')['feature_daily_active_users'].sum().reset_index()

# Plot the active users  trend
plt.figure(figsize=(12, 6))
plt.plot(marketing_trend['date'], marketing_trend['feature_daily_active_users'], marker='o', color='red')
plt.title("Combined Marketing Spend Trend for increasing ARR Companies")
plt.xlabel("Date")
plt.ylabel("active users  ")
plt.grid(True)
plt.tight_layout()
plt.show()

# Sort by date for accurate comparison
df_sorted = df.sort_values('date')

# Identify companies with declining ARR
arr_start_end = df_sorted.groupby('company_name')['arr_usd'].agg(['first', 'last'])
declining_companies = arr_start_end[arr_start_end['last'] < arr_start_end['first']].index

# Filter dataset for declining companies
df_declining = df_sorted[df_sorted['company_name'].isin(declining_companies)]

# Aggregate employees by date
marketing_trend = df_declining.groupby('date')['employees'].sum().reset_index()

# Plot the employees trend
plt.figure(figsize=(12, 6))
plt.plot(marketing_trend['date'], marketing_trend['employees'], marker='o', color='red')
plt.title("Combined employees Trend for Declining ARR Companies")
plt.xlabel("Date")
plt.ylabel("no. of employees")
plt.grid(True)
plt.tight_layout()
plt.show()

# Sort by date for accurate comparison
df_sorted = df.sort_values('date')

# Identify companies with declining ARR
arr_start_end = df_sorted.groupby('company_name')['arr_usd'].agg(['first', 'last'])
inc_companies = arr_start_end[arr_start_end['last'] > arr_start_end['first']].index

# Filter dataset for declining companies
df_inc = df_sorted[df_sorted['company_name'].isin(inc_companies)]

# Aggregate employees by date
marketing_trend = df_inc.groupby('date')['employees'].sum().reset_index()

# Plot the employees trend
plt.figure(figsize=(12, 6))
plt.plot(marketing_trend['date'], marketing_trend['employees'], marker='o', color='red')
plt.title("Combined employees Trend for increasing ARR Companies")
plt.xlabel("Date")
plt.ylabel("no. of employees")
plt.grid(True)
plt.tight_layout()
plt.show()



#heat map of arr increase companies

correlation_matrix_arr_increase = inc[['arr_usd', 'mrr_usd', 'avg_revenue_per_user_usd',
       'new_subscribers', 'churn_rate', 'marketing_spend_usd',
       'feature_daily_active_users', 'employees', 'nps', 'support_tickets']].corr()


plt.figure(figsize=(12, 10))

# Generate a heatmap with annotations
sns.heatmap(correlation_matrix_arr_increase, annot=True, fmt=".2f", cmap="coolwarm", square=True, linewidths=0.5)

# Add title
plt.title("Correlation Matrix Heatmap")

# Show the plot
plt.tight_layout()
plt.show()

#heat map of arr decrease companies

correlation_matrix_arr_decrease = dec[['arr_usd', 'mrr_usd', 'avg_revenue_per_user_usd',
       'new_subscribers', 'churn_rate', 'marketing_spend_usd',
       'feature_daily_active_users', 'employees', 'nps', 'support_tickets']].corr()


plt.figure(figsize=(12, 10))

# Generate a heatmap with annotations
sns.heatmap(correlation_matrix_arr_decrease, annot=True, fmt=".2f", cmap="coolwarm", square=True, linewidths=0.5)

# Add title
plt.title("Correlation Matrix Heatmap")

# Show the plot
plt.tight_layout()
plt.show()



RANDOM_STATE = 42

plt.figure(figsize=(6,4))
sns.scatterplot(x="marketing_spend_usd",
                y="new_subscribers",
                data=df.sample(1800, random_state=RANDOM_STATE),
                alpha=0.6)
plt.title("Acquisition efficiency snapshot")
plt.show()

avg_rev = df['avg_revenue_per_user_usd'].mean()

companies_avg = df.groupby('company_name')['avg_revenue_per_user_usd'].mean().reset_index()
companies_avg

companies_avg = df.groupby('company_name')['avg_revenue_per_user_usd'].mean().reset_index()
abv_avg = companies_avg[companies_avg['avg_revenue_per_user_usd'] >= avg_rev]

print(abv_avg.shape)
abv_avg['company_name']

#heat map of arr decrease companies

correlation_matrix_ab_avg = df[df['company_name'].isin(abv_avg['company_name'])][[
    'arr_usd', 'mrr_usd', 'avg_revenue_per_user_usd',
    'new_subscribers', 'churn_rate', 'marketing_spend_usd',
    'feature_daily_active_users', 'employees', 'nps', 'support_tickets'
]].corr()



# abv_avg[['arr_usd', 'mrr_usd', 'avg_revenue_per_user_usd',
#        'new_subscribers', 'churn_rate', 'marketing_spend_usd',
#        'feature_daily_active_users', 'employees', 'nps', 'support_tickets']].corr()


plt.figure(figsize=(12, 10))

# Generate a heatmap with annotations
sns.heatmap(correlation_matrix_ab_avg, annot=True, fmt=".2f", cmap="coolwarm", square=True, linewidths=0.5)

# Add title
plt.title("Correlation Matrix Heatmap")

# Show the plot
plt.tight_layout()
plt.show()

# companies_avg = df.groupby('company_name')['avg_revenue_per_user_usd'].mean().reset_index()
blw_avg = companies_avg[companies_avg['avg_revenue_per_user_usd'] < avg_rev]
blw_avg['company_name']

print(blw_avg.shape)
blw_avg['company_name']

#heat map of arr decrease companies

correlation_matrix_blw_avg = df[df['company_name'].isin(blw_avg['company_name'])][[
    'arr_usd', 'mrr_usd', 'avg_revenue_per_user_usd',
    'new_subscribers', 'churn_rate', 'marketing_spend_usd',
    'feature_daily_active_users', 'employees', 'nps', 'support_tickets'
]].corr()



# abv_avg[['arr_usd', 'mrr_usd', 'avg_revenue_per_user_usd',
#        'new_subscribers', 'churn_rate', 'marketing_spend_usd',
#        'feature_daily_active_users', 'employees', 'nps', 'support_tickets']].corr()


plt.figure(figsize=(12, 10))

# Generate a heatmap with annotations
sns.heatmap(correlation_matrix_blw_avg, annot=True, fmt=".2f", cmap="coolwarm", square=True, linewidths=0.5)

# Add title
plt.title("Correlation Matrix Heatmap")

# Show the plot
plt.tight_layout()
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Compute average revenue per user across all companies
avg_rev = df['avg_revenue_per_user_usd'].mean()

# 2. Group by company and calculate their average revenue per user
companies_avg = df.groupby('company_name')['avg_revenue_per_user_usd'].mean().reset_index()

# 3. Split into above and below average groups
abv_avg = companies_avg[companies_avg['avg_revenue_per_user_usd'] >= avg_rev]
blw_avg = companies_avg[companies_avg['avg_revenue_per_user_usd'] < avg_rev]

print("Below-average companies:", blw_avg.shape[0])
print("Above-average companies:", abv_avg.shape[0])

# 4. Define the relevant features
features = [
    'arr_usd', 'avg_revenue_per_user_usd',
    'new_subscribers', 'churn_rate', 'marketing_spend_usd',
    'feature_daily_active_users', 'employees', 'nps', 'support_tickets'
]

# 5. Correlation matrices
corr_abv = df[df['company_name'].isin(abv_avg['company_name'])][features].corr()
corr_blw = df[df['company_name'].isin(blw_avg['company_name'])][features].corr()

# 6. Function to list highly correlated pairs
def get_high_corr_pairs(corr_matrix, threshold=0.7):
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)
    upper_triangle = corr_matrix.where(mask)
    pairs = upper_triangle.stack().reset_index()
    pairs.columns = ['Feature 1', 'Feature 2', 'Correlation']
    return pairs[abs(pairs['Correlation']) > threshold].sort_values(by='Correlation', ascending=False)


# 7. Get high correlations for both groups
print("\n--- Highly Correlated Feature Pairs (Above Avg Group) ---")
print(get_high_corr_pairs(corr_abv))

print("\n--- Highly Correlated Feature Pairs (Below Avg Group) ---")
print(get_high_corr_pairs(corr_blw))

# 8. Optional: Heatmaps
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.heatmap(corr_abv, annot=True, cmap='coolwarm', square=True, linewidths=0.5)
plt.title("Correlation Heatmap - Above Avg Companies")

plt.subplot(1, 2, 2)
sns.heatmap(corr_blw, annot=True, cmap='coolwarm', square=True, linewidths=0.5)
plt.title("Correlation Heatmap - Below Avg Companies")

plt.tight_layout()
plt.show()



"""

### 📊 **Feature Correlation Comparison Between Above-Avg and Below-Avg Companies**

| **Feature Pair**                           | **Corr (Above)** | **Corr (Below)** | **Abs Diff** | **Insight**                                                              |
| ------------------------------------------ | ---------------- | ---------------- | ------------ | ------------------------------------------------------------------------ |
| `arr_usd` ↔ `employees`                    | 0.9993           | 0.9994           | 0.0001       | Both groups scale ARR linearly with team size — consistent across all.   |
| `employees` ↔ `support_tickets`            | 0.9401           | 0.7719           | 0.1682       | Above-avg companies scale support ops more tightly with employee count.  |
| `arr_usd` ↔ `support_tickets`              | 0.9400           | 0.7719           | 0.1681       | Stronger link between revenue and support demand in above-avg companies. |
|                                            |                  |                  |              |                                                                          |
| `feature_daily_active_users` ↔ `employees` | 0.8691           | —                | —            | Above-avg companies scale teams with user activity — proactive scaling.  |
| `feature_daily_active_users` ↔ `arr_usd`   | 0.8691           | 0.7299           | 0.1392       | Above-avg companies generate more ARR per active user.                   |
| `marketing_spend_usd` ↔ `employees`        | 0.7850           | 0.8245           | 0.0395       | Below-avg companies invest more in marketing per employee.               |
|                                            |                  |                  |              |                                                                          |
| `marketing_spend_usd` ↔ `support_tickets`  | 0.7354           | —                | —            | Only above-avg companies show ops stress from marketing-driven growth.   |

---

### 🧠 Summary Insights:

* ✅ **Above-Average Companies:**

  * More **strategically aligned** across product usage, team scaling, and revenue.
  * **Support, employees, and ARR** are tightly interlinked.
  * **Engagement metrics** (like daily active users) translate directly into growth and hiring.

* ⚠️ **Below-Average Companies:**

  * Depend slightly more on **marketing spend** to drive revenue.
  * Show **weaker correlations** between user engagement and outcomes (ARR, hiring).
  * May benefit from improving **product monetization and operational alignment**.

---

"""

grp = df.sort_values('date').groupby('company_id')

# Lag features
df['arr_lag_1'] = grp['arr_usd'].shift(1)
df['mrr_lag_1'] = grp['mrr_usd'].shift(1)
df['churn_lag_1'] = grp['churn_rate'].shift(1)

# Rolling stats
df['arr_roll3_mean'] = grp['arr_usd'].rolling(3).mean().reset_index(level=0, drop=True)
df['arr_roll3_std']  = grp['arr_usd'].rolling(3).std().reset_index(level=0, drop=True)

# Growth & efficiency
df['arr_growth_pct'] = grp['arr_usd'].pct_change()
df['marketing_efficiency'] = df['new_subscribers'] / (df['marketing_spend_usd'] + 1)
df['arr_per_employee'] = df['arr_usd'] / df['employees']

# Interaction terms
df['nps_x_churn'] = df['nps'] * df['churn_rate']
df['marketing_x_churn'] = df['marketing_spend_usd'] * df['churn_rate']

# Drop NaNs introduced by lag/rolling
df_fe = df.dropna().reset_index(drop=True)
print("Feature‑engineered shape:", df_fe.shape)

df_fe

df.columns[df.dtypes != 'object']

correlation_matrix_ftr = df[['arr_usd', 'mrr_usd', 'avg_revenue_per_user_usd',
       'new_subscribers', 'churn_rate', 'marketing_spend_usd',
       'feature_daily_active_users', 'employees', 'nps', 'support_tickets',
       'uptime_pct', 'arr_lag_1', 'mrr_lag_1', 'churn_lag_1', 'arr_roll3_mean',
       'arr_roll3_std', 'arr_growth_pct', 'marketing_efficiency',
       'arr_per_employee', 'nps_x_churn', 'marketing_x_churn']].corr()

plt.figure(figsize=(12, 10))

# Generate a heatmap with annotations
sns.heatmap(correlation_matrix_ftr, annot=True, fmt=".2f", cmap="coolwarm", square=True, linewidths=0.5)

# Add title
plt.title("Correlation Matrix Heatmap")

# Show the plot
plt.tight_layout()
plt.show()

df.head(5)

import seaborn as sns

sns.histplot(data = df, x = 'mrr_usd' )

"""Positively Skewed: The tail extends further to the right.
Bimodal: It appears to have two distinct peaks (around 0.2-0.3 and 0.6-0.7 on the x-axis, which likely correspond to 2−3 million and 6−7 million USD, respectively).

"""

from scipy.stats import norm

# mean of the entire population data of mrr
mu = df['mrr_usd'].mean()
mu

sigma = df['mrr_usd'].std()
sigma

sam_100 = df['mrr_usd'].sample(100)

np.mean(sam_100)

sam_100 = [np.mean(sam_100) for i in range(10000) ]

sns.histplot(sam_100, kde=True)

np.random.seed(42)
data_peak1 = np.random.normal(loc=2.5e6, scale=0.8e6, size=600)
data_peak2 = np.random.normal(loc=6.5e6, scale=1.2e6, size=400)
# Combine and ensure positive values, then add some skewness
your_mrr_usd_data = np.concatenate((data_peak1, data_peak2))
your_mrr_usd_data = np.abs(your_mrr_usd_data) # Ensure positive
# Add a slight exponential component to enhance skew
your_mrr_usd_data = your_mrr_usd_data * np.random.exponential(scale=0.8, size=len(your_mrr_usd_data))
your_mrr_usd_data = your_mrr_usd_data[your_mrr_usd_data > 10000] # Remove very small values if any after operations

print(f"Original MRR USD data shape: {your_mrr_usd_data.shape}")
print(f"Original MRR USD min: {np.min(your_mrr_usd_data):.2f}")
print(f"Original MRR USD max: {np.max(your_mrr_usd_data):.2f}")

log_transformed_mrr = np.log(your_mrr_usd_data)

from scipy import stats

try:
    boxcox_transformed_mrr, lambda_boxcox = stats.boxcox(your_mrr_usd_data)
except ValueError:
    print("\nWarning: Box-Cox requires positive data. Adding a small constant and trying again.")
    # Add a small constant if Box-Cox fails due to non-positive values (e.g., if you had 0s)
    boxcox_transformed_mrr, lambda_boxcox = stats.boxcox(your_mrr_usd_data + 1e-9)

yeojohnson_transformed_mrr, lambda_yeojohnson = stats.yeojohnson(your_mrr_usd_data)

plt.figure(figsize=(18, 12)) # Larger figure for better visualization

# Plot 1: Original Data
plt.subplot(2, 2, 1)
sns.histplot(your_mrr_usd_data, kde=True, color='skyblue', bins=30)
plt.title('Original MRR_USD Distribution', fontsize=16)
plt.xlabel('MRR_USD', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.ticklabel_format(style='plain', axis='x') # Prevent scientific notation on x-axis if preferred

# Plot 2: Log Transformed Data
plt.subplot(2, 2, 2)
sns.histplot(log_transformed_mrr, kde=True, color='lightcoral', bins=30)
plt.title('Log Transformed MRR_USD', fontsize=16)
plt.xlabel('Log(MRR_USD)', fontsize=14)
plt.ylabel('Count', fontsize=14)

# Plot 3: Box-Cox Transformed Data
plt.subplot(2, 2, 3)
sns.histplot(boxcox_transformed_mrr, kde=True, color='gold', bins=30)
plt.title(f'Box-Cox Transformed MRR_USD (λ={lambda_boxcox:.2f})', fontsize=16)
plt.xlabel('Box-Cox Transformed Value', fontsize=14)
plt.ylabel('Count', fontsize=14)

plt.subplot(2, 2, 4)
sns.histplot(yeojohnson_transformed_mrr, kde=True, color='mediumpurple', bins=30)
plt.title(f'Yeo-Johnson Transformed MRR_USD (λ={lambda_yeojohnson:.2f})', fontsize=16)
plt.xlabel('Yeo-Johnson Transformed Value', fontsize=14)
plt.ylabel('Count', fontsize=14)

plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to make space for suptitle
plt.suptitle('Comparison of MRR_USD Transformations to Approximate Normality', fontsize=20)
plt.show()

fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('Q-Q Plots for MRR_USD Normality Assessment', y=1.02, fontsize=18)

stats.probplot(your_mrr_usd_data, dist="norm", plot=axes[0, 0])
axes[0, 0].set_title('Original Data Q-Q Plot')

stats.probplot(log_transformed_mrr, dist="norm", plot=axes[0, 1])
axes[0, 1].set_title('Log Transformed Q-Q Plot')

stats.probplot(boxcox_transformed_mrr, dist="norm", plot=axes[1, 0])
axes[1, 0].set_title('Box-Cox Transformed Q-Q Plot')

stats.probplot(yeojohnson_transformed_mrr, dist="norm", plot=axes[1, 1])
axes[1, 1].set_title('Yeo-Johnson Transformed Q-Q Plot')

plt.tight_layout()
plt.show()

print(f"\nOptimal Lambda for Box-Cox: {lambda_boxcox:.4f}")
print(f"Optimal Lambda for Yeo-Johnson: {lambda_yeojohnson:.4f}")